{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jumpr_000\\Anaconda2\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import os, os.path\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from pyqtgraph.Qt import QtCore, QtGui\n",
    "import pyqtgraph.opengl as gl\n",
    "\n",
    "frames_dir = \"kinect_data/\"\n",
    "\n",
    "#camera information based on the Kinect v2 hardware\n",
    "CameraParams = {\n",
    "  \"cx\":254.878,\n",
    "  \"cy\":205.395,\n",
    "  \"fx\":365.456,\n",
    "  \"fy\":365.456,\n",
    "  \"k1\":0.0905474,\n",
    "  \"k2\":-0.26819,\n",
    "  \"k3\":0.0950862,\n",
    "  \"p1\":0.0,\n",
    "  \"p2\":0.0,\n",
    "}\n",
    "\n",
    "# Kinect's physical orientation in the real world.\n",
    "CameraPosition = {\n",
    "    \"x\": 0, # actual position in meters of kinect sensor relative to the viewport's center.\n",
    "    \"y\": 0, # actual position in meters of kinect sensor relative to the viewport's center.\n",
    "    \"z\": 0, # height in meters of actual kinect sensor from the floor.\n",
    "    \"roll\": 0, # angle in degrees of sensor's roll (used for INU input - trig function for this is commented out by default).\n",
    "    \"azimuth\": 0, # sensor's yaw angle in degrees.\n",
    "    \"elevation\": -30, # sensor's pitch angle in degrees.\n",
    "}\n",
    "\n",
    "def depthMatrixToPointCloudPos(z, scale=1000):\n",
    "    #bacically this is a vectorized version of depthToPointCloudPos()\n",
    "    C, R = np.indices(z.shape)\n",
    "\n",
    "    R = np.subtract(R, CameraParams['cx'])\n",
    "    R = np.multiply(R, z)\n",
    "    R = np.divide(R, CameraParams['fx'] * scale)\n",
    "\n",
    "    C = np.subtract(C, CameraParams['cy'])\n",
    "    C = np.multiply(C, z)\n",
    "    C = np.divide(C, CameraParams['fy'] * scale)\n",
    "\n",
    "    return np.column_stack((z.ravel() / scale, R.ravel(), -C.ravel()))\n",
    "\n",
    "def depthToPointCloudPos(x_d, y_d, z, scale=1000):\n",
    "    # This runs in Python slowly as it is required to be called from within a loop, but it is a more intuitive example than it's vertorized alternative (Purly for example)\n",
    "    # calculate the real-world xyz vertex coordinate from the raw depth data (one vertex at a time).\n",
    "    x = (x_d - CameraParams['cx']) * z / CameraParams['fx']\n",
    "    y = (y_d - CameraParams['cy']) * z / CameraParams['fy']\n",
    "\n",
    "    return x / scale, y / scale, z / scale\n",
    "\n",
    "def applyCameraOrientation(pt):\n",
    "    # Kinect Sensor Orientation Compensation\n",
    "    # This runs slowly in Python as it is required to be called within a loop, but it is a more intuitive example than it's vertorized alternative (Purly for example)\n",
    "    # use trig to rotate a vertex around a gimbal.\n",
    "    def rotatePoints(ax1, ax2, deg):\n",
    "        # math to rotate vertexes around a center point on a plane.\n",
    "        hyp = np.sqrt(pt[ax1] ** 2 + pt[ax2] ** 2) # Get the length of the hypotenuse of the real-world coordinate from center of rotation, this is the radius!\n",
    "        d_tan = np.arctan2(pt[ax2], pt[ax1]) # Calculate the vertexes current angle (returns radians that go from -180 to 180)\n",
    "\n",
    "        cur_angle = np.degrees(d_tan) % 360 # Convert radians to degrees and use modulo to adjust range from 0 to 360.\n",
    "        new_angle = np.radians((cur_angle + deg) % 360) # The new angle (in radians) of the vertexes after being rotated by the value of deg.\n",
    "\n",
    "        pt[ax1] = hyp * np.cos(new_angle) # Calculate the rotated coordinate for this axis.\n",
    "        pt[ax2] = hyp * np.sin(new_angle) # Calculate the rotated coordinate for this axis.\n",
    "\n",
    "    #rotatePoints(0, 2, CameraPosition['roll']) #rotate on the Y&Z plane # Disabled because most tripods don't roll. If an Inertial Nav Unit is available this could be used)\n",
    "    rotatePoints(1, 2, CameraPosition['elevation']) #rotate on the X&Z plane\n",
    "    rotatePoints(0, 1, CameraPosition['azimuth']) #rotate on the X&Y plane\n",
    "\n",
    "    # Apply offsets for height and linear position of the sensor (from viewport's center)\n",
    "    pt[:] += np.float_([CameraPosition['x'], CameraPosition['y'], CameraPosition['z']])\n",
    "    return pt\n",
    "\n",
    "def applyCameraMatrixOrientation(pt):\n",
    "    # Kinect Sensor Orientation Compensation\n",
    "    # bacically this is a vectorized version of applyCameraOrientation()\n",
    "    # uses same trig to rotate a vertex around a gimbal.\n",
    "    def rotatePoints(ax1, ax2, deg):\n",
    "        # math to rotate vertexes around a center point on a plane.\n",
    "        hyp = np.sqrt(pt[:, ax1] ** 2 + pt[:, ax2] ** 2) # Get the length of the hypotenuse of the real-world coordinate from center of rotation, this is the radius!\n",
    "        d_tan = np.arctan2(pt[:, ax2], pt[:, ax1]) # Calculate the vertexes current angle (returns radians that go from -180 to 180)\n",
    "\n",
    "        cur_angle = np.degrees(d_tan) % 360 # Convert radians to degrees and use modulo to adjust range from 0 to 360.\n",
    "        new_angle = np.radians((cur_angle + deg) % 360) # The new angle (in radians) of the vertexes after being rotated by the value of deg.\n",
    "\n",
    "        pt[:, ax1] = hyp * np.cos(new_angle) # Calculate the rotated coordinate for this axis.\n",
    "        pt[:, ax2] = hyp * np.sin(new_angle) # Calculate the rotated coordinate for this axis.\n",
    "\n",
    "    #rotatePoints(1, 2, CameraPosition['roll']) #rotate on the Y&Z plane # Disabled because most tripods don't roll. If an Inertial Nav Unit is available this could be used)\n",
    "    rotatePoints(0, 2, CameraPosition['elevation']) #rotate on the X&Z plane\n",
    "    rotatePoints(0, 1, CameraPosition['azimuth']) #rotate on the X&Y\n",
    "\n",
    "    # Apply offsets for height and linear position of the sensor (from viewport's center)\n",
    "    pt[:] += np.float_([CameraPosition['x'], CameraPosition['y'], CameraPosition['z']])\n",
    "    return pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(i):\n",
    "    focal_x = 70.6\n",
    "    depth_frame = np.load(frames_dir + str(i) + \".npy\")\n",
    "    obstacles = np.zeros(depth_frame.shape)\n",
    "    img = depth_frame / 4500.\n",
    "    imgray = np.uint8(img * 255)\n",
    "    sure_bg_dilation = 0 #cv2.getTrackbarPos('bg_dilate', 'depth')\n",
    "    sure_bg_erosion = 2 #cv2.getTrackbarPos('bg_erode', 'depth')\n",
    "    thresh_dilation = 3 #cv2.getTrackbarPos('thresh_dilate', 'depth')\n",
    "    thresh_erosion = 3 #cv2.getTrackbarPos('thresh_erode', 'depth')\n",
    "    opening_iter = 3 #cv2.getTrackbarPos('opening', 'depth')\n",
    "    unknown_erosion = 0 #cv2.getTrackbarPos('unknown_erode', 'depth')\n",
    "    kernel_size = 5 #cv2.getTrackbarPos('kernel_size', 'depth')\n",
    "    blur_amount = 0 #cv2.getTrackbarPos('blur', 'depth')\n",
    "    if blur_amount % 2 == 0:\n",
    "        blur_amount += 1\n",
    "    ret,thresh = cv2.threshold(imgray,0,255,cv2.THRESH_BINARY)\n",
    "\n",
    "    #noise removal\n",
    "    kernel = np.ones((kernel_size,kernel_size),np.uint8)\n",
    "    thresh = cv2.erode(thresh, kernel, iterations = thresh_erosion)\n",
    "    thresh = cv2.dilate(thresh, kernel, iterations = thresh_dilation)\n",
    "    opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations = opening_iter)\n",
    "    #gradient calculation\n",
    "    gradient = cv2.morphologyEx(thresh, cv2.MORPH_GRADIENT, kernel)\n",
    "    dilation = cv2.dilate(thresh,kernel,iterations = 1)\n",
    "    #sure background area\n",
    "    sure_bg = cv2.dilate(opening,kernel,iterations=sure_bg_dilation)\n",
    "    sure_bg = cv2.erode(sure_bg, kernel, iterations = sure_bg_erosion)\n",
    "    dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
    "    ret, sure_fg = cv2.threshold(dist_transform,0.7*dist_transform.max(),255,0)\n",
    "    # Finding unknown region\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    #finding unknown region\n",
    "    #unknown = cv2.subtract(gradient,sure_bg)\n",
    "    unknown = cv2.subtract(sure_bg,sure_fg)\n",
    "    #unknown = cv2.subtract(unknown,gradient)\n",
    "    unknown = cv2.medianBlur(unknown,blur_amount)\n",
    "    unknown = cv2.erode(unknown, kernel, iterations = unknown_erosion)\n",
    "    color = img#cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    #begin contour detection\n",
    "    image, contours, hierarchy = cv2.findContours(sure_bg,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    color = cv2.drawContours(color, contours, -1, (0,255,0), 1)\n",
    "    for cntr in contours:\n",
    "        try:\n",
    "            #calculate diamter of equivalent cirlce\n",
    "            area = cv2.contourArea(cntr)\n",
    "            equi_diameter = np.sqrt(4*area/np.pi)\n",
    "\n",
    "            #Hardcoded Diameter Range in pixels\n",
    "            LOW_DIAMETER_BOUND = 20\n",
    "            HIGH_DIAMETER_BOUND = 150\n",
    "\n",
    "            HIGH_DISTANCE_BOUND = 4500\n",
    "            #Original tolerances were 20 and 150\n",
    "\n",
    "            if(equi_diameter>LOW_DIAMETER_BOUND and equi_diameter<HIGH_DIAMETER_BOUND): #range needs to be tweaked\n",
    "                mask = np.zeros_like(imgray)\n",
    "                ellipse = cv2.fitEllipse(cntr)\n",
    "                x,y,obj_length,obj_height = cv2.boundingRect(cntr)\n",
    "                rect = cv2.minAreaRect(cntr)\n",
    "                \n",
    "                equi_diameter = obj_length\n",
    "                \n",
    "                box = cv2.boxPoints(rect)\n",
    "                box = np.int0(box)\n",
    "                mask = cv2.ellipse(mask,ellipse,(255,255,255),-1)\n",
    "                rows,cols = mask.shape\n",
    "                #shift mask down to match obstacle, not edge\n",
    "                M = np.float32([[1,0,0],[0,1,equi_diameter/4]])\n",
    "                mask = cv2.warpAffine(mask,M,(cols,rows))\n",
    "                mask = cv2.erode(mask, kernel, iterations=3)\n",
    "                img_fg = cv2.bitwise_and(depth_frame,depth_frame,mask = mask)\n",
    "                img_fg = cv2.medianBlur(img_fg,5)\n",
    "                obstacles = cv2.add(np.float32(img_fg), np.float32(obstacles))\n",
    "\n",
    "\n",
    "\n",
    "                # Experimenting with different blur settings\n",
    "                #img_fg = cv2.GaussianBlur(img_fg, (5,5), 0)\n",
    "\n",
    "                #mean_val = cv2.mean(img_fg)[0] #returns mean value of each channel, we only want first channel\n",
    "                non_zero_mean = np.median(img_fg[img_fg.nonzero()])\n",
    "                mean_val = non_zero_mean\n",
    "                min_val, distance_to_object, min_loc, max_loc = cv2.minMaxLoc(img_fg)\n",
    "\n",
    "                moment = cv2.moments(cntr)\n",
    "                cx = int(moment['m10']/moment['m00'])\n",
    "                cy = int(moment['m01']/moment['m00'])\n",
    "\n",
    "                if mean_val < HIGH_DISTANCE_BOUND:\n",
    "                    coords = depthToPointCloudPos(cx, cy, mean_val)\n",
    "                    #coords = applyCameraOrientation(coords)\n",
    "                    \n",
    "                    mm_diameter = (equi_diameter) * (1.0 / CameraParams['fx']) * mean_val\n",
    "\n",
    "                    img = cv2.ellipse(color,ellipse,(0,255,0),2)\n",
    "                    cv2.drawContours(color,[box],0,(0,0,255),1)\n",
    "                    cv2.rectangle(color,(x,y),(x+obj_length,y+obj_height),(0,255,0),2)\n",
    "                    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "                    cv2.putText(img, \"x\" + str(coords[0]), (cx,cy+30), font, 0.4, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(img, \"y\" + str(coords[1]), (cx,cy+45), font, 0.4, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(color, \"z\" + str(mean_val), (cx,cy+60), font, 0.4, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "                    cv2.putText(color,\"diameter = \" + str(mm_diameter), (cx,cy + 15), font, 0.4, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        except:\n",
    "            print (\"Failed to fit ellipse\")\n",
    "\n",
    "    \n",
    "    cv2.imshow(\"unknown\", unknown)\n",
    "    cv2.imshow(\"obstacles\", obstacles)\n",
    "    cv2.imshow(\"depth\", img)\n",
    "    cv2.imwrite(\"original_depth_frame.png\",depth_frame)\n",
    "    cv2.imwrite(\"threshold.png\",thresh)\n",
    "    cv2.imwrite(\"opening.png\",opening)\n",
    "    cv2.imwrite(\"sure_bg.png\",sure_bg)\n",
    "    cv2.imwrite(\"distance_transform.png\",dist_transform)\n",
    "    cv2.imwrite(\"thresholded_dist_transform.png\",sure_fg)\n",
    "    cv2.imwrite(\"unknown.png\",unknown)\n",
    "    cv2.imwrite(\"processed_frame.png\",img)\n",
    "    key = cv2.waitKey(delay=1)\n",
    "    #time.sleep(0.1)\n",
    "    \n",
    "    return obstacles\n",
    "\n",
    "def plot_cloud(i, sp2, obstacles):\n",
    "    depth_frame = np.load(frames_dir + str(i) + \".npy\")\n",
    "    img = depth_frame / 4500.\n",
    "    xyz_arr = depthMatrixToPointCloudPos(depth_frame)\n",
    "    xyz_arr = applyCameraMatrixOrientation(xyz_arr)\n",
    "    obstacles_arr = depthMatrixToPointCloudPos(obstacles)\n",
    "    obstacles_arr = applyCameraMatrixOrientation(obstacles_arr)\n",
    "    colors = ((1.0, 1.0, 1.0, 1.0))\n",
    "    colors = np.uint8(obstacles_arr)\n",
    "    #colors = np.divide(colors, 255)\n",
    "    #colors = colors.reshape(colors.shape[0] * colors.shape[1], 4 )\n",
    "    colors = colors[:, :3:] #BGRA to BGR (slices out the alpha channel)  \n",
    "    colors = colors[...,::-1] #BGR to RGB\n",
    "    # Calculate a dynamic vertex size based on window dimensions and camera's position - To become the \"size\" input for the scatterplot's setData() function.\n",
    "    v_rate = 5.0 # Rate that vertex sizes will increase as zoom level increases (adjust this to any desired value).\n",
    "    v_scale = np.float32(v_rate) / gl_widget.opts['distance'] # Vertex size increases as the camera is \"zoomed\" towards center of view.\n",
    "    v_offset = (gl_widget.geometry().width() / 1000)**2 # Vertex size is offset based on actual width of the viewport.\n",
    "    v_size = v_scale + v_offset\n",
    "    #cloud = PyntCloud(points)\n",
    "    #cloud.plot(point_size=0.05, opacity=0.6)\n",
    "\n",
    "    # Show the data in a scatter plot\n",
    "    sp2.setData(pos=xyz_arr, color=colors, size=v_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plane_fit(points):\n",
    "    \"\"\"\n",
    "    p, n = planeFit(points)\n",
    "\n",
    "    Given an array, points, of shape (d,...)\n",
    "    representing points in d-dimensional space,\n",
    "    fit an d-dimensional plane to the points.\n",
    "    Return a point, p, on the plane (the point-cloud centroid),\n",
    "    and the normal, n.\n",
    "    \"\"\"\n",
    "    from numpy.linalg import svd\n",
    "    points = np.reshape(points, (np.shape(points)[0], -1)) # Collapse trialing dimensions\n",
    "    assert points.shape[0] <= points.shape[1], \"There are only {} points in {} dimensions.\".format(points.shape[1], points.shape[0])\n",
    "    ctr = points.mean(axis=1)\n",
    "    x = points - ctr[:,np.newaxis]\n",
    "    M = np.dot(x, x.T) # Could also use np.cov(x) here.\n",
    "    return ctr, svd(M)[0][:,-1]\n",
    "\n",
    "def get_orientation(xyz_arr, n_iter):\n",
    "    spherical = []\n",
    "    planes = []\n",
    "    centers = []\n",
    "    for _ in range(0, n_iter):\n",
    "        rand_points = []\n",
    "        while len(rand_points) < 10:\n",
    "            index = random.randrange(0,len(xyz_arr))\n",
    "            if not xyz_arr[index].all() == np.zeros(3).all():\n",
    "                rand_points.append(xyz_arr[index])\n",
    "        rand_points = np.array(rand_points).T\n",
    "        ctr, P = plane_fit(rand_points)\n",
    "        r = math.sqrt(P[0]**2 + P[1]**2 + P[2]**2)\n",
    "        theta = math.acos(P[2]/r) * 180 / math.pi\n",
    "        phi = math.atan(P[1]/P[0]) * 180 / math.pi\n",
    "        spherical.append(theta)\n",
    "        planes.append(P)\n",
    "        centers.append(ctr)\n",
    "    return np.mean(centers, axis = 0), np.mean(planes, axis = 0), np.mean(spherical)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import os, os.path\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from pyqtgraph.Qt import QtCore, QtGui\n",
    "import pyqtgraph.opengl as gl\n",
    "from pyntcloud import PyntCloud\n",
    "from pyntcloud.scalar_fields.base import ScalarField\n",
    "from pyntcloud.scalar_fields.xyz import XYZScalarField\n",
    "from pyntcloud.scalar_fields.xyz import PlaneFit\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def get_obstacles_with_plane(depth_frame, num_planes, max_distance):\n",
    "    obstacles = np.zeros(depth_frame.shape) #empty image that will store the locations of detected obstacles\n",
    "    img = np.uint8(depth_frame) #some opencv functions break when you pass a float array\n",
    "\n",
    "    xyz_arr = depthMatrixToPointCloudPos(depth_frame) #convert depth data to XYZ coordinates\n",
    "    print(xyz_arr.shape)\n",
    "    center, plane, theta = get_orientation(xyz_arr, num_planes)\n",
    "    CameraPosition['elevation'] = -theta\n",
    "    center = applyCameraOrientation(center)\n",
    "    plane = applyCameraOrientation(plane)\n",
    "    xyz_arr = applyCameraMatrixOrientation(xyz_arr)\n",
    "    #xyz_arr = np.array(filter(lambda x: x[1] > 0.3 + center[1], xyz_arr)) \n",
    "    #xyz_arr = xyz_arr[np.where(xyz_arr[2] > 0.3 - center[2])]\n",
    "    #filtered = []\n",
    "    #for point in xyz_arr:\n",
    "    #    if point[2] > 0.1 - center[2]:\n",
    "    #        filtered.append(point)\n",
    "    plane_img = np.zeros(len(xyz_arr))\n",
    "    plane_img[xyz_arr[:,2] > 0.12 - center[2]] = 1\n",
    "    xyz_arr = xyz_arr[xyz_arr[:,2] > 0.12 - center[2]]\n",
    "    points = pd.DataFrame(xyz_arr, columns=['x', 'y', 'z']) #convert XYZ coordinates to a DataFrame for pyntcloud\n",
    "    cloud = PyntCloud(points)\n",
    "    \n",
    "    lines = [\n",
    "        {\n",
    "            \"color\": \"red\",\n",
    "            \"vertices\": [[0, 0, 0], [10, 0, 0]]\n",
    "        },\n",
    "        {\n",
    "            \"color\": \"green\",\n",
    "            \"vertices\": [[0, 0, 0], [0, 10, 0]]\n",
    "        },\n",
    "        {\n",
    "            \"color\": \"blue\",\n",
    "            \"vertices\": [[0, 0, 0], [0, 0, 10]]\n",
    "        },\n",
    "        {\n",
    "            \"color\": \"pink\",\n",
    "            \"vertices\": [center.tolist(), plane.tolist()]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    \n",
    "\n",
    "    #start_time = time.time()\n",
    "    #planes = [] #this will store the fitted planes\n",
    "    #for i in range(0, num_planes):\n",
    "    #    is_plane = cloud.add_scalar_field(\"plane_fit\", max_dist=max_distance) #perform RANSAC plane fitting\n",
    "    #    planes.append(np.float32(points['is_plane'])) #plane fitting adds an 'is_plane' column to our DataFrame \n",
    "    #duration = time.time() - start_time\n",
    "    #print(\"Fitting took \" + str(duration) + \" seconds\")\n",
    "\n",
    "    #cloud.plot(use_as_color=is_plane, cmap=\"cool\", polylines=lines)\n",
    "    cloud.plot(cmap=\"cool\", polylines=lines)\n",
    "\n",
    "    #plane_img = np.mean(planes, axis = 0) #take the pixel average across all detected planes\n",
    "    plane_img = np.uint8(np.reshape(plane_img,(424,512)) * 255) #reshape to match depth data and convert to uint8\n",
    "    plane_img = np.uint8((np.ones((424,512)) * 255) - plane_img) #invert img so pixel value corresponds to NOT ground plane\n",
    "    #plane_confidence = 0.75 #percentage of fitted planes that must have contained a certain point\n",
    "    ret, plane_img = cv2.threshold(plane_img,0,255,cv2.THRESH_BINARY) #filter points that are probaly not ground plane\n",
    "    plane_img = cv2.subtract(img, plane_img)\n",
    "    #noise removal\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    opening = cv2.morphologyEx(plane_img ,cv2.MORPH_OPEN, kernel, iterations = 3) #erosion followed by dilation\n",
    "    #sure_bg = cv2.erode(plane_img, kernel, iterations = 3) #further erosion to isolate obstacles\n",
    "\n",
    "    color = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR) #BGR image to draw labels on\n",
    "\n",
    "    #begin contour detection\n",
    "    image, contours, hierarchy = cv2.findContours(opening,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    color = cv2.drawContours(color, contours, -1, (0,255,0), 1)\n",
    "    for cntr in contours:\n",
    "        try:\n",
    "            #calculate diamter of equivalent circle\n",
    "            #this measurement is only used for checking if countours fit our bounds\n",
    "            area = cv2.contourArea(cntr)\n",
    "            equi_diameter = np.sqrt(4*area/np.pi)\n",
    "\n",
    "            #Hardcoded Diameter Range in pixels\n",
    "            LOW_DIAMETER_BOUND = 20\n",
    "            HIGH_DIAMETER_BOUND = 150\n",
    "\n",
    "            HIGH_DISTANCE_BOUND = 4500\n",
    "            #Original tolerances were 20 and 150\n",
    "\n",
    "            if(equi_diameter>LOW_DIAMETER_BOUND and equi_diameter<HIGH_DIAMETER_BOUND):\n",
    "                mask = np.zeros_like(img) #mask will contain the fitted and adjusted ellipse of a single obstacle\n",
    "                ellipse = cv2.fitEllipse(cntr)\n",
    "                x,y,obj_length,obj_height = cv2.boundingRect(cntr)\n",
    "                rect = cv2.minAreaRect(cntr)\n",
    "\n",
    "                equi_diameter = obj_length #bounding rectangle gives a better approximation of diameter\n",
    "\n",
    "                box = cv2.boxPoints(rect)\n",
    "                box = np.int0(box)\n",
    "                mask = cv2.ellipse(mask,ellipse,(255,255,255),-1) #draw the fitted ellipse\n",
    "                rows = mask.shape[0]\n",
    "                cols = mask.shape[1]\n",
    "                M = np.float32([[1,0,0],[0,1,equi_diameter/4]]) #shift mask down to match obstacle, not edge\n",
    "                mask = cv2.warpAffine(mask,M,(cols,rows))\n",
    "                mask = cv2.erode(mask, kernel, iterations=3) #erode the mask to remove background points\n",
    "                img_fg = cv2.bitwise_and(depth_frame,depth_frame,mask = mask) #use the mask to isolate original depth values\n",
    "                img_fg = cv2.medianBlur(img_fg,5) #median blur to further remove noise\n",
    "                obstacles = cv2.add(np.float32(img_fg), np.float32(obstacles)) \n",
    "                \n",
    "                mean_val = np.median(img_fg[img_fg.nonzero()]) #compute the non-zero average of obstacle depth values\n",
    "                min_val, distance_to_object, min_loc, max_loc = cv2.minMaxLoc(img_fg)\n",
    "\n",
    "                moment = cv2.moments(cntr) #get the centroid of the obstacle using its moment\n",
    "                cx = int(moment['m10']/moment['m00'])\n",
    "                cy = int(moment['m01']/moment['m00'])\n",
    "\n",
    "                if mean_val < HIGH_DISTANCE_BOUND: #kinect loses accuracy beyond 4.5m\n",
    "                    coords = depthToPointCloudPos(cx, cy, mean_val) #convert obstacle depth to XYZ coordinate\n",
    "\n",
    "                    mm_diameter = (equi_diameter) * (1.0 / CameraParams['fx']) * mean_val #convert pixel diameter to mm\n",
    "\n",
    "                    #begin visualization\n",
    "                    cv2.drawContours(color,[box],0,(0,0,255),1)\n",
    "                    cv2.rectangle(color,(x,y),(x+obj_length,y+obj_height),(0,255,0),2)\n",
    "                    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                    cv2.putText(color, \"x\" + str(coords[0]), (cx,cy+30), font, 0.4, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(color, \"y\" + str(coords[1]), (cx,cy+45), font, 0.4, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(color, \"z\" + str(mean_val), (cx,cy+60), font, 0.4, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(color,\"diameter = \" + str(mm_diameter), (cx,cy + 15), font, 0.4, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        except:\n",
    "            print (\"Failed to process image\")\n",
    "            print (sys.exc_info()[0])\n",
    "\n",
    "    cv2.imshow(\"plane\",plane_img)\n",
    "    cv2.imshow(\"img\",depth_frame)\n",
    "    cv2.imshow(\"final\",color)\n",
    "    cv2.imshow(\"sure_bg\",opening)\n",
    "    cv2.imshow(\"obstacles\",obstacles)\n",
    "    key = cv2.waitKey(delay=1)\n",
    "    return color\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217088, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jumpr_000\\Anaconda2\\envs\\py36\\lib\\site-packages\\pythreejs\\traits.py:175: UserWarning: 64-bit data types not supported for WebGL data, casting to 32-bit.\n",
      "  warnings.warn('64-bit data types not supported for WebGL '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303afc6850544622836392223d28ffcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.6, fov=90.0, position=(0.009014453423769152, 0.49499956275137735, -…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849aa076020247ccba15fb1010a1db8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Point size:'), FloatSlider(value=0.2438675166477815, max=2.438675166477815, step=0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = np.load('kinect_data/116.npy')\n",
    "output = get_obstacles_with_plane(img, num_planes = 5, max_distance = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jumpr_000\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: The Windows bytes API has been deprecated, use Unicode filenames instead\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\jumpr_000\\Anaconda2\\envs\\py36\\lib\\site-packages\\pyntcloud\\geometry\\models\\plane.py:17: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.normal = normal / np.linalg.norm(normal)\n",
      "C:\\Users\\jumpr_000\\Anaconda2\\envs\\py36\\lib\\site-packages\\pyntcloud\\ransac\\fitters.py:74: RuntimeWarning: invalid value encountered in less_equal\n",
      "  inliers = all_distances <= model.max_dist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kinect_data/0.npy\n",
      "Fitting took 5.690698146820068 seconds\n",
      "kinect_data/1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jumpr_000\\AppData\\Roaming\\Python\\Python35\\site-packages\\numpy\\core\\fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\jumpr_000\\AppData\\Roaming\\Python\\Python35\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\jumpr_000\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:95: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting took 5.6419665813446045 seconds\n",
      "kinect_data/10.npy\n",
      "Fitting took 5.554112434387207 seconds\n",
      "kinect_data/100.npy\n",
      "Fitting took 5.603052139282227 seconds\n",
      "kinect_data/101.npy\n",
      "Fitting took 5.572788953781128 seconds\n",
      "kinect_data/102.npy\n",
      "Fitting took 5.562612533569336 seconds\n",
      "kinect_data/103.npy\n",
      "Fitting took 5.557041168212891 seconds\n",
      "kinect_data/104.npy\n",
      "Fitting took 5.572796821594238 seconds\n",
      "kinect_data/105.npy\n",
      "Fitting took 5.569042682647705 seconds\n",
      "kinect_data/106.npy\n",
      "Fitting took 5.573739051818848 seconds\n",
      "kinect_data/107.npy\n",
      "Fitting took 5.570455312728882 seconds\n",
      "kinect_data/108.npy\n",
      "Fitting took 5.547644853591919 seconds\n",
      "kinect_data/109.npy\n",
      "Fitting took 5.5537683963775635 seconds\n",
      "kinect_data/11.npy\n",
      "Fitting took 5.590807676315308 seconds\n",
      "kinect_data/110.npy\n",
      "Fitting took 5.55189061164856 seconds\n",
      "kinect_data/111.npy\n",
      "Fitting took 5.567464828491211 seconds\n",
      "kinect_data/112.npy\n",
      "Fitting took 5.553461074829102 seconds\n",
      "kinect_data/113.npy\n",
      "Fitting took 5.545739412307739 seconds\n",
      "kinect_data/114.npy\n",
      "Fitting took 5.612223863601685 seconds\n",
      "kinect_data/115.npy\n",
      "Fitting took 5.580135822296143 seconds\n",
      "kinect_data/116.npy\n",
      "Fitting took 5.5660319328308105 seconds\n",
      "kinect_data/117.npy\n",
      "Fitting took 5.57661509513855 seconds\n",
      "kinect_data/118.npy\n",
      "Fitting took 5.563736915588379 seconds\n",
      "kinect_data/119.npy\n",
      "Fitting took 5.557798147201538 seconds\n",
      "kinect_data/12.npy\n",
      "Fitting took 5.568293809890747 seconds\n",
      "kinect_data/120.npy\n",
      "Fitting took 5.583890199661255 seconds\n",
      "kinect_data/121.npy\n",
      "Fitting took 5.567205190658569 seconds\n",
      "kinect_data/122.npy\n",
      "Fitting took 5.555799245834351 seconds\n",
      "kinect_data/123.npy\n",
      "Fitting took 5.571777820587158 seconds\n",
      "kinect_data/124.npy\n",
      "Fitting took 5.584061145782471 seconds\n",
      "kinect_data/125.npy\n",
      "Fitting took 5.56368088722229 seconds\n",
      "kinect_data/126.npy\n",
      "Fitting took 5.5766921043396 seconds\n",
      "kinect_data/127.npy\n",
      "Fitting took 5.581605434417725 seconds\n",
      "kinect_data/128.npy\n",
      "Fitting took 5.692937850952148 seconds\n",
      "kinect_data/129.npy\n",
      "Fitting took 5.567828893661499 seconds\n",
      "kinect_data/13.npy\n",
      "Fitting took 5.6717894077301025 seconds\n",
      "kinect_data/130.npy\n",
      "Fitting took 5.5758748054504395 seconds\n",
      "kinect_data/131.npy\n",
      "Fitting took 5.555276155471802 seconds\n",
      "kinect_data/132.npy\n",
      "Fitting took 5.565865755081177 seconds\n",
      "kinect_data/133.npy\n",
      "Fitting took 5.622568845748901 seconds\n",
      "kinect_data/134.npy\n",
      "Fitting took 5.572400093078613 seconds\n",
      "kinect_data/135.npy\n",
      "Fitting took 5.571655750274658 seconds\n",
      "kinect_data/136.npy\n",
      "Fitting took 5.572366952896118 seconds\n",
      "kinect_data/137.npy\n",
      "Fitting took 5.769925594329834 seconds\n",
      "kinect_data/138.npy\n",
      "Fitting took 5.67214298248291 seconds\n",
      "kinect_data/139.npy\n",
      "Fitting took 5.570921897888184 seconds\n",
      "kinect_data/14.npy\n",
      "Fitting took 5.569570541381836 seconds\n",
      "kinect_data/140.npy\n",
      "Fitting took 5.608468055725098 seconds\n",
      "kinect_data/141.npy\n",
      "Fitting took 5.697917938232422 seconds\n",
      "kinect_data/142.npy\n",
      "Fitting took 5.630141973495483 seconds\n",
      "kinect_data/143.npy\n",
      "Fitting took 5.566893577575684 seconds\n",
      "kinect_data/144.npy\n",
      "Fitting took 5.6817426681518555 seconds\n",
      "kinect_data/145.npy\n",
      "Fitting took 5.665753602981567 seconds\n",
      "kinect_data/146.npy\n",
      "Fitting took 5.733478307723999 seconds\n",
      "kinect_data/147.npy\n",
      "Fitting took 5.711562871932983 seconds\n",
      "kinect_data/148.npy\n",
      "Fitting took 5.586366176605225 seconds\n",
      "kinect_data/149.npy\n",
      "Fitting took 5.606524705886841 seconds\n",
      "kinect_data/15.npy\n",
      "Fitting took 5.615547180175781 seconds\n",
      "kinect_data/150.npy\n",
      "Fitting took 5.590608596801758 seconds\n",
      "kinect_data/151.npy\n",
      "Fitting took 5.578094244003296 seconds\n",
      "kinect_data/152.npy\n",
      "Fitting took 5.660173654556274 seconds\n",
      "kinect_data/153.npy\n",
      "Fitting took 5.585238933563232 seconds\n",
      "kinect_data/154.npy\n",
      "Fitting took 5.617625713348389 seconds\n",
      "kinect_data/155.npy\n",
      "Fitting took 5.573184251785278 seconds\n",
      "kinect_data/156.npy\n",
      "Fitting took 5.598243236541748 seconds\n",
      "kinect_data/157.npy\n",
      "Fitting took 5.5875818729400635 seconds\n",
      "kinect_data/158.npy\n",
      "Fitting took 5.589310884475708 seconds\n",
      "kinect_data/159.npy\n",
      "Fitting took 5.57751202583313 seconds\n",
      "kinect_data/16.npy\n",
      "Fitting took 5.593879222869873 seconds\n",
      "kinect_data/160.npy\n",
      "Fitting took 5.586063861846924 seconds\n",
      "kinect_data/161.npy\n",
      "Fitting took 5.584524869918823 seconds\n",
      "kinect_data/162.npy\n",
      "Fitting took 5.601297378540039 seconds\n",
      "kinect_data/163.npy\n",
      "Fitting took 5.574872255325317 seconds\n",
      "kinect_data/164.npy\n",
      "Fitting took 5.583523511886597 seconds\n",
      "kinect_data/165.npy\n",
      "Fitting took 5.5751214027404785 seconds\n",
      "kinect_data/166.npy\n",
      "Fitting took 5.608958721160889 seconds\n",
      "kinect_data/167.npy\n",
      "Fitting took 5.647700309753418 seconds\n",
      "kinect_data/168.npy\n",
      "Fitting took 5.58823561668396 seconds\n",
      "kinect_data/169.npy\n",
      "Fitting took 5.578496932983398 seconds\n",
      "kinect_data/17.npy\n",
      "Fitting took 5.585092067718506 seconds\n",
      "kinect_data/170.npy\n",
      "Fitting took 5.5844292640686035 seconds\n",
      "kinect_data/171.npy\n",
      "Fitting took 5.611286163330078 seconds\n",
      "kinect_data/172.npy\n",
      "Fitting took 5.59970760345459 seconds\n",
      "kinect_data/173.npy\n",
      "Fitting took 5.600508213043213 seconds\n",
      "kinect_data/174.npy\n",
      "Fitting took 5.572228670120239 seconds\n",
      "kinect_data/175.npy\n",
      "Fitting took 5.587790489196777 seconds\n",
      "kinect_data/176.npy\n",
      "Fitting took 5.59762167930603 seconds\n",
      "kinect_data/177.npy\n",
      "Fitting took 5.5831687450408936 seconds\n",
      "kinect_data/178.npy\n",
      "Fitting took 5.6767356395721436 seconds\n",
      "kinect_data/179.npy\n",
      "Fitting took 5.647981405258179 seconds\n",
      "kinect_data/18.npy\n",
      "Fitting took 5.598081827163696 seconds\n",
      "kinect_data/180.npy\n",
      "Fitting took 5.700791597366333 seconds\n",
      "kinect_data/181.npy\n",
      "Fitting took 5.775317430496216 seconds\n",
      "kinect_data/182.npy\n",
      "Fitting took 5.971465826034546 seconds\n",
      "kinect_data/183.npy\n",
      "Fitting took 5.734556436538696 seconds\n",
      "kinect_data/184.npy\n",
      "Fitting took 5.641859292984009 seconds\n",
      "kinect_data/185.npy\n",
      "Fitting took 5.783089637756348 seconds\n",
      "kinect_data/186.npy\n",
      "Fitting took 5.887596607208252 seconds\n",
      "kinect_data/187.npy\n",
      "Fitting took 5.773513317108154 seconds\n",
      "kinect_data/188.npy\n",
      "Fitting took 5.872408866882324 seconds\n",
      "kinect_data/189.npy\n",
      "Fitting took 5.944647312164307 seconds\n",
      "kinect_data/19.npy\n",
      "Fitting took 5.964906930923462 seconds\n",
      "kinect_data/190.npy\n",
      "Fitting took 5.626057386398315 seconds\n",
      "kinect_data/191.npy\n",
      "Fitting took 5.82033371925354 seconds\n",
      "kinect_data/192.npy\n",
      "Fitting took 5.783315420150757 seconds\n",
      "kinect_data/193.npy\n",
      "Fitting took 5.635949373245239 seconds\n",
      "kinect_data/194.npy\n",
      "Fitting took 5.6649181842803955 seconds\n",
      "kinect_data/195.npy\n",
      "Fitting took 5.793046951293945 seconds\n",
      "kinect_data/196.npy\n",
      "Fitting took 5.611199140548706 seconds\n",
      "kinect_data/197.npy\n",
      "Fitting took 5.593426704406738 seconds\n",
      "kinect_data/198.npy\n",
      "Fitting took 5.5944318771362305 seconds\n",
      "kinect_data/199.npy\n",
      "Fitting took 5.609112977981567 seconds\n",
      "kinect_data/2.npy\n",
      "Fitting took 5.594112396240234 seconds\n",
      "kinect_data/20.npy\n",
      "Fitting took 5.592514753341675 seconds\n",
      "kinect_data/200.npy\n",
      "Fitting took 5.598830461502075 seconds\n",
      "kinect_data/201.npy\n",
      "Fitting took 5.572847843170166 seconds\n",
      "kinect_data/202.npy\n",
      "Fitting took 5.622101545333862 seconds\n",
      "kinect_data/203.npy\n",
      "Fitting took 5.554318904876709 seconds\n",
      "kinect_data/204.npy\n",
      "Fitting took 5.5566184520721436 seconds\n",
      "kinect_data/205.npy\n",
      "Fitting took 5.557192325592041 seconds\n",
      "kinect_data/206.npy\n",
      "Fitting took 5.611222267150879 seconds\n",
      "kinect_data/207.npy\n",
      "Fitting took 5.561725854873657 seconds\n",
      "kinect_data/208.npy\n",
      "Fitting took 5.655668020248413 seconds\n",
      "kinect_data/209.npy\n",
      "Fitting took 5.586799383163452 seconds\n",
      "kinect_data/21.npy\n",
      "Fitting took 5.565267086029053 seconds\n",
      "kinect_data/210.npy\n",
      "Fitting took 5.5693066120147705 seconds\n",
      "kinect_data/211.npy\n",
      "Fitting took 5.579899072647095 seconds\n",
      "kinect_data/212.npy\n",
      "Fitting took 5.583775758743286 seconds\n",
      "kinect_data/213.npy\n",
      "Fitting took 5.594163179397583 seconds\n",
      "kinect_data/214.npy\n",
      "Fitting took 5.763923645019531 seconds\n",
      "kinect_data/215.npy\n",
      "Fitting took 5.70614767074585 seconds\n",
      "kinect_data/216.npy\n",
      "Fitting took 5.689071893692017 seconds\n",
      "kinect_data/217.npy\n",
      "Fitting took 5.650424480438232 seconds\n",
      "kinect_data/218.npy\n",
      "Fitting took 5.665870904922485 seconds\n",
      "kinect_data/219.npy\n",
      "Fitting took 5.672517538070679 seconds\n",
      "kinect_data/22.npy\n",
      "Fitting took 5.6448869705200195 seconds\n",
      "kinect_data/220.npy\n",
      "Fitting took 5.712292909622192 seconds\n",
      "kinect_data/221.npy\n",
      "Fitting took 5.662377119064331 seconds\n",
      "kinect_data/222.npy\n",
      "Fitting took 5.599464178085327 seconds\n",
      "kinect_data/223.npy\n",
      "Fitting took 5.649715900421143 seconds\n",
      "kinect_data/224.npy\n",
      "Fitting took 5.693005323410034 seconds\n",
      "kinect_data/225.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting took 5.657552480697632 seconds\n",
      "kinect_data/226.npy\n",
      "Fitting took 5.852157115936279 seconds\n",
      "kinect_data/227.npy\n",
      "Fitting took 5.769652605056763 seconds\n",
      "kinect_data/228.npy\n",
      "Fitting took 5.82677698135376 seconds\n",
      "kinect_data/229.npy\n",
      "Fitting took 5.86987566947937 seconds\n",
      "kinect_data/23.npy\n",
      "Fitting took 5.737469434738159 seconds\n",
      "kinect_data/24.npy\n",
      "Fitting took 5.6930625438690186 seconds\n",
      "kinect_data/25.npy\n",
      "Fitting took 5.71571159362793 seconds\n",
      "kinect_data/26.npy\n",
      "Fitting took 5.653802871704102 seconds\n",
      "kinect_data/27.npy\n",
      "Fitting took 5.843183517456055 seconds\n",
      "kinect_data/28.npy\n",
      "Fitting took 5.921947002410889 seconds\n",
      "kinect_data/29.npy\n",
      "Fitting took 5.771690845489502 seconds\n",
      "kinect_data/3.npy\n",
      "Fitting took 5.772683143615723 seconds\n",
      "kinect_data/30.npy\n",
      "Fitting took 5.95632266998291 seconds\n",
      "kinect_data/31.npy\n",
      "Fitting took 5.884305000305176 seconds\n",
      "kinect_data/32.npy\n",
      "Fitting took 5.96824049949646 seconds\n",
      "kinect_data/33.npy\n",
      "Fitting took 5.906722545623779 seconds\n",
      "kinect_data/34.npy\n",
      "Fitting took 5.754024267196655 seconds\n",
      "kinect_data/35.npy\n",
      "Fitting took 5.716635227203369 seconds\n",
      "kinect_data/36.npy\n",
      "Fitting took 5.733646869659424 seconds\n",
      "kinect_data/37.npy\n",
      "Fitting took 5.786563396453857 seconds\n",
      "kinect_data/38.npy\n",
      "Fitting took 5.772538661956787 seconds\n",
      "kinect_data/39.npy\n",
      "Fitting took 5.864394187927246 seconds\n",
      "kinect_data/4.npy\n",
      "Fitting took 5.915670871734619 seconds\n",
      "kinect_data/40.npy\n",
      "Fitting took 5.882438898086548 seconds\n",
      "kinect_data/41.npy\n",
      "Fitting took 5.880504846572876 seconds\n",
      "kinect_data/42.npy\n",
      "Fitting took 5.876710891723633 seconds\n",
      "kinect_data/43.npy\n",
      "Fitting took 6.135984897613525 seconds\n",
      "kinect_data/44.npy\n",
      "Fitting took 5.955284118652344 seconds\n",
      "kinect_data/45.npy\n",
      "Fitting took 5.808748960494995 seconds\n",
      "kinect_data/46.npy\n",
      "Fitting took 5.845592737197876 seconds\n",
      "kinect_data/47.npy\n",
      "Fitting took 6.024484634399414 seconds\n",
      "kinect_data/48.npy\n",
      "Fitting took 6.0021326541900635 seconds\n",
      "kinect_data/49.npy\n",
      "Fitting took 5.971744775772095 seconds\n",
      "kinect_data/5.npy\n",
      "Fitting took 5.977075815200806 seconds\n",
      "kinect_data/50.npy\n",
      "Fitting took 5.903227806091309 seconds\n",
      "kinect_data/51.npy\n",
      "Fitting took 5.906749486923218 seconds\n",
      "kinect_data/52.npy\n",
      "Fitting took 5.9191014766693115 seconds\n",
      "kinect_data/53.npy\n",
      "Fitting took 5.837687730789185 seconds\n",
      "kinect_data/54.npy\n",
      "Fitting took 5.734727144241333 seconds\n",
      "kinect_data/55.npy\n",
      "Fitting took 5.830432415008545 seconds\n",
      "kinect_data/56.npy\n",
      "Fitting took 5.814631700515747 seconds\n",
      "kinect_data/57.npy\n",
      "Fitting took 5.861255884170532 seconds\n",
      "kinect_data/58.npy\n",
      "Fitting took 5.8706066608428955 seconds\n",
      "kinect_data/59.npy\n",
      "Fitting took 6.454620599746704 seconds\n",
      "kinect_data/6.npy\n",
      "Fitting took 5.859678030014038 seconds\n",
      "kinect_data/60.npy\n",
      "Fitting took 5.901524543762207 seconds\n",
      "kinect_data/61.npy\n",
      "Fitting took 5.865979194641113 seconds\n",
      "kinect_data/62.npy\n",
      "Fitting took 5.725150108337402 seconds\n",
      "kinect_data/63.npy\n",
      "Fitting took 5.681229829788208 seconds\n",
      "kinect_data/64.npy\n",
      "Fitting took 5.680560827255249 seconds\n",
      "kinect_data/65.npy\n",
      "Fitting took 5.695291996002197 seconds\n",
      "kinect_data/66.npy\n",
      "Fitting took 5.668866395950317 seconds\n",
      "kinect_data/67.npy\n",
      "Fitting took 5.6826255321502686 seconds\n",
      "kinect_data/68.npy\n",
      "Fitting took 5.663074970245361 seconds\n",
      "kinect_data/69.npy\n",
      "Fitting took 5.655731439590454 seconds\n",
      "kinect_data/7.npy\n",
      "Fitting took 5.705515146255493 seconds\n",
      "kinect_data/70.npy\n",
      "Fitting took 5.688030481338501 seconds\n",
      "kinect_data/71.npy\n",
      "Fitting took 5.681890964508057 seconds\n",
      "kinect_data/72.npy\n",
      "Fitting took 5.664900064468384 seconds\n",
      "kinect_data/73.npy\n",
      "Fitting took 5.647789478302002 seconds\n",
      "kinect_data/74.npy\n",
      "Fitting took 5.720868110656738 seconds\n",
      "kinect_data/75.npy\n",
      "Fitting took 5.791977405548096 seconds\n",
      "kinect_data/76.npy\n",
      "Fitting took 5.689441442489624 seconds\n",
      "kinect_data/77.npy\n",
      "Fitting took 5.735448598861694 seconds\n",
      "kinect_data/78.npy\n",
      "Fitting took 5.671808481216431 seconds\n",
      "kinect_data/79.npy\n",
      "Fitting took 5.654886722564697 seconds\n",
      "kinect_data/8.npy\n",
      "Fitting took 5.7495362758636475 seconds\n",
      "kinect_data/80.npy\n",
      "Fitting took 5.760498046875 seconds\n",
      "kinect_data/81.npy\n",
      "Fitting took 5.745875835418701 seconds\n",
      "kinect_data/82.npy\n",
      "Fitting took 5.70087456703186 seconds\n",
      "kinect_data/83.npy\n",
      "Fitting took 5.694624662399292 seconds\n",
      "kinect_data/84.npy\n",
      "Fitting took 5.700490951538086 seconds\n",
      "kinect_data/85.npy\n",
      "Fitting took 5.707046031951904 seconds\n",
      "kinect_data/86.npy\n",
      "Fitting took 5.700739860534668 seconds\n",
      "kinect_data/87.npy\n",
      "Fitting took 5.712666988372803 seconds\n",
      "kinect_data/88.npy\n",
      "Fitting took 5.684168100357056 seconds\n",
      "kinect_data/89.npy\n",
      "Fitting took 5.702226877212524 seconds\n",
      "kinect_data/9.npy\n",
      "Fitting took 5.727760076522827 seconds\n",
      "kinect_data/90.npy\n",
      "Fitting took 5.837896108627319 seconds\n",
      "kinect_data/91.npy\n",
      "Fitting took 5.676185846328735 seconds\n",
      "kinect_data/92.npy\n",
      "Fitting took 5.708722114562988 seconds\n",
      "kinect_data/93.npy\n",
      "Fitting took 5.752518177032471 seconds\n",
      "kinect_data/94.npy\n",
      "Fitting took 5.808923006057739 seconds\n",
      "kinect_data/95.npy\n",
      "Fitting took 5.8380959033966064 seconds\n",
      "kinect_data/96.npy\n",
      "Fitting took 5.882088899612427 seconds\n",
      "kinect_data/97.npy\n",
      "Fitting took 5.739004373550415 seconds\n",
      "kinect_data/98.npy\n",
      "Fitting took 5.700089454650879 seconds\n",
      "kinect_data/99.npy\n",
      "Fitting took 5.713591575622559 seconds\n"
     ]
    }
   ],
   "source": [
    "frames_dir = 'kinect_data/'\n",
    "directory = os.fsencode(frames_dir)\n",
    "frame_i = 0\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".npy\"):\n",
    "        file_path = frames_dir + filename\n",
    "        print(file_path)\n",
    "        img = np.load(file_path)\n",
    "        output = get_obstacles_with_plane(img, num_planes = 5, max_distance = 0.1)\n",
    "        cv2.imwrite(str(frame_i) + '.png', output)\n",
    "        frame_i += 1\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def update():\n",
    "    for i in range(0,229):\n",
    "        print(i)\n",
    "        obstacles = process_frame(i)\n",
    "        plot_cloud(i,sp2,obstacles)\n",
    "def update_image(i):\n",
    "    plot_cloud(175,sp2,process_frame(175))\n",
    "        \n",
    "cv2.namedWindow('depth')\n",
    "\n",
    "#cv2.createTrackbar('bg_dilate', 'depth', 0, 10, update_image) # 0\n",
    "#cv2.createTrackbar('bg_erode', 'depth', 0, 10, update_image) # 2\n",
    "#cv2.createTrackbar('thresh_dilate', 'depth', 0, 10, update_image) # 3\n",
    "#cv2.createTrackbar('thresh_erode', 'depth', 0, 10, update_image) # 3\n",
    "#cv2.createTrackbar('opening', 'depth', 0, 10, update_image) # 3\n",
    "#cv2.createTrackbar('unknown_erode', 'depth', 0, 10, update_image) # 0\n",
    "#cv2.createTrackbar('kernel_size', 'depth', 0, 10, update_image) # 5\n",
    "#cv2.createTrackbar('blur', 'depth', 0, 10, update_image) # 0\n",
    "\n",
    "#QT app\n",
    "app = QtGui.QApplication([])\n",
    "gl_widget = gl.GLViewWidget()\n",
    "gl_widget.show()\n",
    "gl_grid = gl.GLGridItem()\n",
    "gl_widget.addItem(gl_grid)\n",
    "#initialize some points data\n",
    "pos = np.zeros((1,3))\n",
    "\n",
    "\n",
    "sp2 = gl.GLScatterPlotItem(pos=pos)\n",
    "sp2.setGLOptions('opaque') # Ensures not to allow vertexes located behinde other vertexes to be seen.\n",
    "\n",
    "gl_widget.addItem(sp2)\n",
    "\n",
    "t = QtCore.QTimer()\n",
    "#t.timeout.connect(update)\n",
    "t.start(50)\n",
    "\n",
    "process_frame(200)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    if (sys.flags.interactive != 1) or not hasattr(QtCore, 'PYQT_VERSION'):\n",
    "        QtGui.QApplication.instance().exec_()\n",
    "\n",
    "sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw Frame Viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "frames_dir = \"test_frames_5_6/\"\n",
    "\n",
    "directory = os.fsencode(frames_dir)\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".npy\"):\n",
    "        cv2.imshow(\"test\",np.load(frames_dir + '/'+ filename))\n",
    "        key = cv2.waitKey(delay=30)\n",
    "        print(filename)\n",
    "        time.sleep(0.5)\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
